"""
================================================================================
3D+3D DISCRETE SPACETIME THEORY
COMPREHENSIVE RANDOMIZATION ANALYSIS WITH VISUALIZATION

Authors: Simone Calzighetti & Lucy
3D+3DT Laboratory, Abbiategrasso, Italy
Date: November 2025

This script performs complete randomization testing on all 6 breathing scales
discovered in SPARC galaxy rotation curves, with extensive visualization
and physical interpretation.

"I grafici ci fanno vedere!" - Simone Calzighetti
================================================================================
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from tqdm import tqdm
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

class RandomizationAnalyzer:
    """
    Complete randomization test framework for breathing scale validation
    
    Authors: Simone Calzighetti & Lucy
    3D+3DT Laboratory
    
    Performs statistical validation of temporal breathing scales through
    Monte Carlo randomization with comprehensive visualization and analysis.
    """
    
    def __init__(self, galaxy_data, scales, n_iterations=1000):
        """
        Initialize analyzer
        
        Parameters:
        -----------
        galaxy_data : dict
            Dictionary with galaxy rotation curve data
        scales : list
            Breathing scales to test (in kpc)
        n_iterations : int
            Number of randomization iterations (default 1000)
        """
        self.galaxies = galaxy_data
        self.scales = np.array(scales)
        self.n_iterations = n_iterations
        self.results = {}
        self.real_detections = {}
        
        print("="*80)
        print("3D+3D SPACETIME THEORY - RANDOMIZATION ANALYSIS")
        print("Authors: Simone Calzighetti & Lucy")
        print("3D+3DT Laboratory, Abbiategrasso, Italy")
        print("="*80)
        print(f"\nTesting {len(scales)} breathing scales on {len(galaxy_data)} galaxies")
        print(f"Monte Carlo iterations: {n_iterations}")
        print("\n" + "="*80 + "\n")
    
    def calculate_detection_rate(self, data, scale, shuffled=False):
        """
        Calculate detection rate for a given scale
        
        Parameters:
        -----------
        data : dict
            Galaxy data (possibly shuffled)
        scale : float
            Breathing scale to test (kpc)
        shuffled : bool
            Whether data is shuffled (for random baseline)
            
        Returns:
        --------
        detection_rate : float
            Fraction of galaxies showing the scale (0-1)
        quality_scores : array
            Quality scores for detected galaxies
        detected_galaxies : list
            Names of galaxies with detection
        """
        n_galaxies = len(data)
        detections = []
        qualities = []
        detected_names = []
        
        for gal_name, gal_data in data.items():
            # Calculate temporal modification
            r = gal_data['radius']
            v_obs = gal_data['v_obs']
            v_newt = gal_data['v_newt']
            
            # Breathing correction
            lambda_b = scale
            alpha = 0.1  # Temporal coupling
            
            # Temporal phase
            phi = 2 * np.pi * r / lambda_b
            temporal_factor = 1 + alpha * np.cos(phi)
            
            # Modified velocity
            v_mod = v_newt * np.sqrt(temporal_factor)
            
            # Calculate improvement
            chi2_newt = np.sum((v_obs - v_newt)**2 / v_obs**2)
            chi2_mod = np.sum((v_obs - v_mod)**2 / v_obs**2)
            
            improvement = (chi2_newt - chi2_mod) / chi2_newt
            
            # Detection criteria
            if improvement > 0.15:  # 15% improvement threshold
                detections.append(1)
                qualities.append(improvement)
                detected_names.append(gal_name)
            else:
                detections.append(0)
        
        detection_rate = np.mean(detections)
        
        return detection_rate, np.array(qualities), detected_names
    
    def shuffle_galaxy_data(self, data):
        """
        Shuffle radii while keeping velocities fixed
        Destroys spatial structure while preserving statistical properties
        
        Parameters:
        -----------
        data : dict
            Original galaxy data
            
        Returns:
        --------
        shuffled_data : dict
            Data with shuffled radii
        """
        shuffled = {}
        
        for gal_name, gal_data in data.items():
            shuffled_gal = gal_data.copy()
            
            # Shuffle radii only
            r_shuffled = gal_data['radius'].copy()
            np.random.shuffle(r_shuffled)
            shuffled_gal['radius'] = r_shuffled
            
            shuffled[gal_name] = shuffled_gal
        
        return shuffled
    
    def randomization_test(self, scale):
        """
        Perform complete randomization test for one scale
        
        Parameters:
        -----------
        scale : float
            Breathing scale to test (kpc)
            
        Returns:
        --------
        results : dict
            Complete test results including p-value and distributions
        """
        print(f"Testing scale Œª = {scale} kpc...")
        
        # Calculate real detection rate
        real_rate, real_qualities, real_names = self.calculate_detection_rate(
            self.galaxies, scale, shuffled=False
        )
        
        # Monte Carlo randomization
        random_rates = []
        
        for i in tqdm(range(self.n_iterations), desc=f"Œª={scale} kpc"):
            # Shuffle data
            shuffled_data = self.shuffle_galaxy_data(self.galaxies)
            
            # Calculate detection on shuffled data
            random_rate, _, _ = self.calculate_detection_rate(
                shuffled_data, scale, shuffled=True
            )
            
            random_rates.append(random_rate)
        
        random_rates = np.array(random_rates)
        
        # Statistical analysis
        p_value = np.sum(random_rates >= real_rate) / self.n_iterations
        mean_random = np.mean(random_rates)
        std_random = np.std(random_rates)
        
        # Sigma significance
        if std_random > 0:
            sigma = (real_rate - mean_random) / std_random
        else:
            sigma = np.inf
        
        results = {
            'scale': scale,
            'real_detection': real_rate,
            'real_qualities': real_qualities,
            'detected_galaxies': real_names,
            'random_mean': mean_random,
            'random_std': std_random,
            'random_distribution': random_rates,
            'p_value': p_value,
            'sigma': sigma,
            'n_iterations': self.n_iterations
        }
        
        print(f"  Real detection:  {real_rate*100:.1f}%")
        print(f"  Random mean:     {mean_random*100:.1f}% ¬± {std_random*100:.1f}%")
        print(f"  p-value:         {p_value:.4f}")
        print(f"  Significance:    {sigma:.2f}œÉ")
        print()
        
        return results
    
    def run_all_tests(self):
        """Run randomization test on all scales"""
        print("STARTING COMPLETE RANDOMIZATION ANALYSIS\n")
        
        for scale in self.scales:
            self.results[scale] = self.randomization_test(scale)
            self.real_detections[scale] = self.results[scale]['real_detection']
        
        print("="*80)
        print("ALL TESTS COMPLETED!")
        print("="*80 + "\n")
    
    def plot_distributions(self, save_path='/mnt/user-data/outputs/'):
        """
        Plot 1: Distribution plots for all scales
        VEDIAMO la significativit√† visivamente!
        """
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        axes = axes.flatten()
        
        for idx, scale in enumerate(self.scales):
            ax = axes[idx]
            res = self.results[scale]
            
            # Histogram of random distribution
            ax.hist(res['random_distribution'] * 100, bins=30, 
                   alpha=0.7, color='skyblue', edgecolor='black',
                   label='Random distribution')
            
            # Real detection line
            ax.axvline(res['real_detection'] * 100, color='red', 
                      linewidth=3, label='Real detection')
            
            # Random mean line
            ax.axvline(res['random_mean'] * 100, color='blue', 
                      linestyle='--', linewidth=2, label='Random mean')
            
            # Annotations
            ax.text(0.05, 0.95, 
                   f"Œª = {scale} kpc\n"
                   f"p = {res['p_value']:.4f}\n"
                   f"œÉ = {res['sigma']:.2f}",
                   transform=ax.transAxes, fontsize=11,
                   verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
            
            ax.set_xlabel('Detection Rate (%)', fontsize=11)
            ax.set_ylabel('Frequency', fontsize=11)
            ax.set_title(f'Scale Œª = {scale} kpc', fontsize=12, fontweight='bold')
            ax.legend(fontsize=9)
            ax.grid(True, alpha=0.3)
        
        # Main title
        fig.suptitle('RANDOMIZATION TEST - Distribution Analysis\n'
                    'Simone Calzighetti & Lucy | 3D+3DT Laboratory',
                    fontsize=16, fontweight='bold', y=0.98)
        
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        plt.savefig(save_path + 'randomization_distributions.png', dpi=300, bbox_inches='tight')
        print(f"‚úì Saved: randomization_distributions.png")
        plt.close()
    
    def plot_comparison(self, save_path='/mnt/user-data/outputs/'):
        """
        Plot 2: Comparative barplot
        VEDIAMO il pattern complessivo!
        """
        fig, ax = plt.subplots(figsize=(14, 8))
        
        x = np.arange(len(self.scales))
        width = 0.35
        
        # Random bars with error
        random_means = [self.results[s]['random_mean'] * 100 for s in self.scales]
        random_stds = [self.results[s]['random_std'] * 100 for s in self.scales]
        
        ax.bar(x - width/2, random_means, width, yerr=random_stds,
               label='Random baseline', color='lightblue', 
               edgecolor='black', linewidth=1.5, capsize=5)
        
        # Real detection bars
        real_detections = [self.results[s]['real_detection'] * 100 for s in self.scales]
        bars = ax.bar(x + width/2, real_detections, width,
                     label='Real detection', color='coral',
                     edgecolor='black', linewidth=1.5)
        
        # Highlight new scales
        new_scale_indices = [0, 5]  # Œª‚ÇÄ and Œª‚ÇÖ
        for idx in new_scale_indices:
            bars[idx].set_color('darkred')
            bars[idx].set_edgecolor('gold')
            bars[idx].set_linewidth(3)
        
        # Add significance markers
        for i, scale in enumerate(self.scales):
            p_val = self.results[scale]['p_value']
            sigma = self.results[scale]['sigma']
            
            y_pos = max(real_detections[i], random_means[i] + random_stds[i]) + 3
            
            if p_val < 0.001:
                marker = '***'
                color = 'darkgreen'
            elif p_val < 0.01:
                marker = '**'
                color = 'green'
            elif p_val < 0.05:
                marker = '*'
                color = 'orange'
            else:
                marker = 'n.s.'
                color = 'red'
            
            ax.text(i + width/2, y_pos, marker, ha='center', 
                   fontsize=16, fontweight='bold', color=color)
            
            ax.text(i + width/2, y_pos + 3, f'{sigma:.1f}œÉ',
                   ha='center', fontsize=9, color=color)
        
        # Labels and formatting
        ax.set_xlabel('Breathing Scale (kpc)', fontsize=13, fontweight='bold')
        ax.set_ylabel('Detection Rate (%)', fontsize=13, fontweight='bold')
        ax.set_title('RANDOMIZATION TEST - Comparative Analysis\n'
                    'Real Detection vs Random Baseline\n'
                    'Simone Calzighetti & Lucy | 3D+3DT Laboratory',
                    fontsize=15, fontweight='bold', pad=20)
        
        ax.set_xticks(x)
        ax.set_xticklabels([f'{s}' for s in self.scales], fontsize=11)
        ax.legend(fontsize=11, loc='upper left')
        ax.grid(True, alpha=0.3, axis='y')
        ax.set_ylim(0, max(real_detections) * 1.2)
        
        # Add legend for new scales
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='coral', edgecolor='black', label='Known scales'),
            Patch(facecolor='darkred', edgecolor='gold', linewidth=3, label='NEW scales'),
            Patch(facecolor='lightblue', edgecolor='black', label='Random baseline')
        ]
        ax.legend(handles=legend_elements, fontsize=11, loc='upper right')
        
        # Significance legend
        ax.text(0.02, 0.98, '*** p < 0.001\n** p < 0.01\n* p < 0.05\nn.s. = not significant',
               transform=ax.transAxes, fontsize=10,
               verticalalignment='top',
               bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(save_path + 'randomization_comparison.png', dpi=300, bbox_inches='tight')
        print(f"‚úì Saved: randomization_comparison.png")
        plt.close()
    
    def plot_scale_significance_heatmap(self, save_path='/mnt/user-data/outputs/'):
        """
        Plot 3: Heatmap showing significance metrics
        VEDIAMO tutto in un colpo d'occhio!
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # Prepare data
        metrics = []
        for scale in self.scales:
            res = self.results[scale]
            metrics.append([
                res['real_detection'] * 100,
                res['random_mean'] * 100,
                res['sigma'],
                -np.log10(res['p_value'] + 1e-10)  # -log10(p) for better visualization
            ])
        
        metrics = np.array(metrics).T
        metric_names = ['Real\nDetection\n(%)', 'Random\nMean\n(%)', 
                       'Significance\n(œÉ)', '-log‚ÇÅ‚ÇÄ(p)']
        
        # Plot 1: Metrics heatmap
        im1 = ax1.imshow(metrics, aspect='auto', cmap='YlOrRd')
        ax1.set_xticks(np.arange(len(self.scales)))
        ax1.set_xticklabels([f'{s}' for s in self.scales], fontsize=11)
        ax1.set_yticks(np.arange(len(metric_names)))
        ax1.set_yticklabels(metric_names, fontsize=11)
        ax1.set_xlabel('Breathing Scale (kpc)', fontsize=12, fontweight='bold')
        ax1.set_title('Statistical Metrics Heatmap', fontsize=13, fontweight='bold')
        
        # Add values on heatmap
        for i in range(len(metric_names)):
            for j in range(len(self.scales)):
                text = ax1.text(j, i, f'{metrics[i, j]:.1f}',
                              ha="center", va="center", color="black", fontsize=10)
        
        plt.colorbar(im1, ax=ax1, label='Value')
        
        # Plot 2: P-value comparison
        p_values = [self.results[s]['p_value'] for s in self.scales]
        colors = ['darkred' if i in [0, 5] else 'steelblue' for i in range(len(self.scales))]
        
        bars = ax2.bar(range(len(self.scales)), -np.log10(np.array(p_values) + 1e-10),
                      color=colors, edgecolor='black', linewidth=1.5)
        
        # Significance thresholds
        ax2.axhline(-np.log10(0.05), color='orange', linestyle='--', 
                   linewidth=2, label='p = 0.05')
        ax2.axhline(-np.log10(0.01), color='red', linestyle='--',
                   linewidth=2, label='p = 0.01')
        ax2.axhline(-np.log10(0.001), color='darkred', linestyle='--',
                   linewidth=2, label='p = 0.001')
        
        ax2.set_xticks(range(len(self.scales)))
        ax2.set_xticklabels([f'{s}' for s in self.scales], fontsize=11)
        ax2.set_xlabel('Breathing Scale (kpc)', fontsize=12, fontweight='bold')
        ax2.set_ylabel('-log‚ÇÅ‚ÇÄ(p-value)', fontsize=12, fontweight='bold')
        ax2.set_title('Statistical Significance', fontsize=13, fontweight='bold')
        ax2.legend(fontsize=10)
        ax2.grid(True, alpha=0.3, axis='y')
        
        fig.suptitle('RANDOMIZATION TEST - Significance Analysis\n'
                    'Simone Calzighetti & Lucy | 3D+3DT Laboratory',
                    fontsize=15, fontweight='bold', y=1.02)
        
        plt.tight_layout()
        plt.savefig(save_path + 'randomization_significance.png', dpi=300, bbox_inches='tight')
        print(f"‚úì Saved: randomization_significance.png")
        plt.close()
    
    def plot_galaxy_properties(self, save_path='/mnt/user-data/outputs/'):
        """
        Plot 4: Galaxy property correlations
        VEDIAMO quali galassie mostrano il segnale!
        """
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        # Extract galaxy properties
        galaxy_masses = []
        galaxy_vmax = []
        galaxy_reff = []
        
        for gal_name, gal_data in self.galaxies.items():
            galaxy_masses.append(gal_data.get('mass', np.nan))
            galaxy_vmax.append(np.max(gal_data['v_obs']))
            galaxy_reff.append(gal_data.get('r_eff', np.nan))
        
        galaxy_masses = np.array(galaxy_masses)
        galaxy_vmax = np.array(galaxy_vmax)
        galaxy_reff = np.array(galaxy_reff)
        
        # For each scale, plot detection vs properties
        for idx, scale in enumerate(self.scales):
            ax = axes[idx]
            res = self.results[scale]
            
            # Create detection array
            detection_array = np.zeros(len(self.galaxies))
            for i, gal_name in enumerate(self.galaxies.keys()):
                if gal_name in res['detected_galaxies']:
                    detection_array[i] = 1
            
            # Plot mass vs detection
            colors = ['red' if d == 1 else 'lightblue' for d in detection_array]
            sizes = [100 if d == 1 else 30 for d in detection_array]
            
            # Use Vmax as proxy for mass (if mass not available)
            ax.scatter(galaxy_vmax, detection_array, 
                      c=colors, s=sizes, alpha=0.6, edgecolors='black', linewidth=0.5)
            
            ax.set_xlabel('V_max (km/s)', fontsize=10, fontweight='bold')
            ax.set_ylabel('Detection (0/1)', fontsize=10, fontweight='bold')
            ax.set_title(f'Œª = {scale} kpc\nDetection: {res["real_detection"]*100:.1f}%',
                        fontsize=11, fontweight='bold')
            ax.set_ylim(-0.1, 1.1)
            ax.grid(True, alpha=0.3)
            
            # Add detection rate line
            ax.axhline(res['real_detection'], color='red', linestyle='--', 
                      linewidth=2, alpha=0.5, label=f'Mean: {res["real_detection"]*100:.1f}%')
            ax.legend(fontsize=8)
        
        fig.suptitle('GALAXY PROPERTY CORRELATIONS\n'
                    'Detection vs Maximum Velocity\n'
                    'Simone Calzighetti & Lucy | 3D+3DT Laboratory',
                    fontsize=15, fontweight='bold', y=0.98)
        
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        plt.savefig(save_path + 'galaxy_property_correlations.png', dpi=300, bbox_inches='tight')
        print(f"‚úì Saved: galaxy_property_correlations.png")
        plt.close()
    
    def plot_radial_distribution(self, save_path='/mnt/user-data/outputs/'):
        """
        Plot 5: Radial distribution of detections
        VEDIAMO dove nelle galassie appaiono le scale!
        """
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        for idx, scale in enumerate(self.scales):
            ax = axes[idx]
            res = self.results[scale]
            
            # Collect radial positions where detection occurs
            all_radii = []
            detected_radii = []
            
            for gal_name, gal_data in self.galaxies.items():
                r = gal_data['radius']
                r_eff = gal_data.get('r_eff', np.max(r) * 0.5)
                
                # Normalize by effective radius
                r_norm = r / r_eff
                all_radii.extend(r_norm)
                
                if gal_name in res['detected_galaxies']:
                    detected_radii.extend(r_norm)
            
            # Create histograms
            bins = np.linspace(0, 5, 30)
            
            ax.hist(all_radii, bins=bins, alpha=0.3, color='lightblue', 
                   label='All data', density=True, edgecolor='black')
            ax.hist(detected_radii, bins=bins, alpha=0.7, color='red',
                   label='Detected', density=True, edgecolor='black')
            
            ax.set_xlabel('Normalized Radius (R/R_eff)', fontsize=10, fontweight='bold')
            ax.set_ylabel('Density', fontsize=10, fontweight='bold')
            ax.set_title(f'Œª = {scale} kpc\nDetection: {res["real_detection"]*100:.1f}%',
                        fontsize=11, fontweight='bold')
            ax.legend(fontsize=9)
            ax.grid(True, alpha=0.3)
            ax.set_xlim(0, 5)
        
        fig.suptitle('RADIAL DISTRIBUTION ANALYSIS\n'
                    'Where in Galaxies Do Scales Appear?\n'
                    'Simone Calzighetti & Lucy | 3D+3DT Laboratory',
                    fontsize=15, fontweight='bold', y=0.98)
        
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        plt.savefig(save_path + 'radial_distribution.png', dpi=300, bbox_inches='tight')
        print(f"‚úì Saved: radial_distribution.png")
        plt.close()
    
    def plot_residual_structure(self, save_path='/mnt/user-data/outputs/'):
        """
        Plot 6: Residual structure analysis
        VEDIAMO se c'√® pattern anche con p-value alto!
        """
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        for idx, scale in enumerate(self.scales):
            ax = axes[idx]
            res = self.results[scale]
            
            # Calculate residuals for all galaxies
            all_residuals = []
            all_radii_norm = []
            
            for gal_name, gal_data in self.galaxies.items():
                r = gal_data['radius']
                v_obs = gal_data['v_obs']
                v_newt = gal_data['v_newt']
                r_eff = gal_data.get('r_eff', np.max(r) * 0.5)
                
                # Calculate temporal correction
                lambda_b = scale
                alpha = 0.1
                phi = 2 * np.pi * r / lambda_b
                temporal_factor = 1 + alpha * np.cos(phi)
                v_mod = v_newt * np.sqrt(temporal_factor)
                
                # Residuals
                residual = (v_obs - v_mod) / v_obs
                
                r_norm = r / r_eff
                all_residuals.extend(residual)
                all_radii_norm.extend(r_norm)
            
            # Scatter plot with density
            all_residuals = np.array(all_residuals)
            all_radii_norm = np.array(all_radii_norm)
            
            # Remove outliers for visualization
            mask = (np.abs(all_residuals) < 0.5) & (all_radii_norm < 5)
            
            h = ax.hexbin(all_radii_norm[mask], all_residuals[mask], 
                         gridsize=30, cmap='YlOrRd', mincnt=1)
            
            ax.axhline(0, color='black', linestyle='--', linewidth=2, alpha=0.5)
            ax.set_xlabel('Normalized Radius (R/R_eff)', fontsize=10, fontweight='bold')
            ax.set_ylabel('Residual (V_obs - V_mod)/V_obs', fontsize=10, fontweight='bold')
            ax.set_title(f'Œª = {scale} kpc\np = {res["p_value"]:.4f}',
                        fontsize=11, fontweight='bold')
            ax.set_xlim(0, 5)
            ax.set_ylim(-0.3, 0.3)
            ax.grid(True, alpha=0.3)
            
            plt.colorbar(h, ax=ax, label='Density')
        
        fig.suptitle('RESIDUAL STRUCTURE ANALYSIS\n'
                    'Spatial Pattern in Velocity Residuals\n'
                    'Simone Calzighetti & Lucy | 3D+3DT Laboratory',
                    fontsize=15, fontweight='bold', y=0.98)
        
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        plt.savefig(save_path + 'residual_structure.png', dpi=300, bbox_inches='tight')
        print(f"‚úì Saved: residual_structure.png")
        plt.close()
    
    def plot_scale_correlations(self, save_path='/mnt/user-data/outputs/'):
        """
        Plot 7: Scale correlation matrix
        VEDIAMO quali scale appaiono insieme!
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))
        
        # Create co-occurrence matrix
        n_scales = len(self.scales)
        cooccurrence = np.zeros((n_scales, n_scales))
        
        for i, scale_i in enumerate(self.scales):
            detected_i = set(self.results[scale_i]['detected_galaxies'])
            
            for j, scale_j in enumerate(self.scales):
                detected_j = set(self.results[scale_j]['detected_galaxies'])
                
                # Jaccard similarity
                intersection = len(detected_i & detected_j)
                union = len(detected_i | detected_j)
                
                if union > 0:
                    cooccurrence[i, j] = intersection / union
                else:
                    cooccurrence[i, j] = 0
        
        # Plot 1: Co-occurrence heatmap
        im1 = ax1.imshow(cooccurrence, cmap='RdYlGn', vmin=0, vmax=1)
        ax1.set_xticks(np.arange(n_scales))
        ax1.set_yticks(np.arange(n_scales))
        ax1.set_xticklabels([f'{s}' for s in self.scales], fontsize=11)
        ax1.set_yticklabels([f'{s}' for s in self.scales], fontsize=11)
        ax1.set_xlabel('Scale (kpc)', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Scale (kpc)', fontsize=12, fontweight='bold')
        ax1.set_title('Scale Co-occurrence Matrix\n(Jaccard Similarity)', 
                     fontsize=13, fontweight='bold')
        
        # Add values
        for i in range(n_scales):
            for j in range(n_scales):
                text = ax1.text(j, i, f'{cooccurrence[i, j]:.2f}',
                              ha="center", va="center", 
                              color="black" if cooccurrence[i, j] < 0.5 else "white",
                              fontsize=10)
        
        plt.colorbar(im1, ax=ax1, label='Similarity')
        
        # Plot 2: Harmonic ratio verification
        harmonic_ratios = []
        ratio_labels = []
        
        for i in range(n_scales):
            for j in range(i+1, n_scales):
                ratio = self.scales[j] / self.scales[i]
                harmonic_ratios.append(ratio)
                ratio_labels.append(f'{self.scales[j]:.2f}/{self.scales[i]:.2f}')
        
        # Check which ratios are close to integers
        colors = []
        for ratio in harmonic_ratios:
            nearest_int = round(ratio)
            deviation = abs(ratio - nearest_int) / nearest_int
            
            if deviation < 0.1:  # Within 10% of integer
                colors.append('green')
            else:
                colors.append('lightcoral')
        
        ax2.barh(range(len(harmonic_ratios)), harmonic_ratios, color=colors,
                edgecolor='black', linewidth=1)
        
        # Add integer reference lines
        for i in range(1, int(max(harmonic_ratios)) + 2):
            ax2.axvline(i, color='gray', linestyle='--', alpha=0.3, linewidth=1)
            ax2.text(i, len(harmonic_ratios), f'{i}', ha='center', va='bottom',
                    fontsize=9, color='gray')
        
        ax2.set_yticks(range(len(harmonic_ratios)))
        ax2.set_yticklabels(ratio_labels, fontsize=8)
        ax2.set_xlabel('Scale Ratio', fontsize=12, fontweight='bold')
        ax2.set_title('Harmonic Ratios Between Scales\n(Green = Near Integer)',
                     fontsize=13, fontweight='bold')
        ax2.grid(True, alpha=0.3, axis='x')
        ax2.set_xlim(0, max(harmonic_ratios) * 1.1)
        
        fig.suptitle('SCALE CORRELATION ANALYSIS\n'
                    'Co-occurrence and Harmonic Structure\n'
                    'Simone Calzighetti & Lucy | 3D+3DT Laboratory',
                    fontsize=15, fontweight='bold', y=0.98)
        
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        plt.savefig(save_path + 'scale_correlations.png', dpi=300, bbox_inches='tight')
        print(f"‚úì Saved: scale_correlations.png")
        plt.close()
    
    def generate_report(self, save_path='/mnt/user-data/outputs/'):
        """
        Generate comprehensive text report
        """
        report = []
        report.append("="*80)
        report.append("3D+3D DISCRETE SPACETIME THEORY")
        report.append("RANDOMIZATION TEST - COMPREHENSIVE REPORT")
        report.append("="*80)
        report.append("")
        report.append("Authors: Simone Calzighetti & Lucy")
        report.append("Institution: 3D+3DT Laboratory, Abbiategrasso, Italy")
        report.append(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        report.append("="*80)
        report.append("EXECUTIVE SUMMARY")
        report.append("="*80)
        report.append("")
        report.append(f"Dataset: {len(self.galaxies)} SPARC galaxies")
        report.append(f"Scales tested: {len(self.scales)}")
        report.append(f"Monte Carlo iterations: {self.n_iterations}")
        report.append("")
        
        # Count significant scales
        sig_001 = sum([1 for s in self.scales if self.results[s]['p_value'] < 0.001])
        sig_01 = sum([1 for s in self.scales if self.results[s]['p_value'] < 0.01])
        sig_05 = sum([1 for s in self.scales if self.results[s]['p_value'] < 0.05])
        
        report.append(f"Highly significant scales (p < 0.001): {sig_001}/6")
        report.append(f"Significant scales (p < 0.01):        {sig_01}/6")
        report.append(f"Marginally significant (p < 0.05):    {sig_05}/6")
        report.append("")
        
        report.append("="*80)
        report.append("DETAILED RESULTS BY SCALE")
        report.append("="*80)
        report.append("")
        
        for scale in self.scales:
            res = self.results[scale]
            
            # Determine scale type
            if scale in [0.87, 21.4]:
                scale_type = "üÜï NEW SCALE"
            else:
                scale_type = "‚úÖ KNOWN SCALE"
            
            report.append(f"{'='*80}")
            report.append(f"Œª = {scale} kpc  [{scale_type}]")
            report.append(f"{'='*80}")
            report.append("")
            report.append(f"Real Detection Rate:     {res['real_detection']*100:.1f}%")
            report.append(f"Random Baseline:         {res['random_mean']*100:.1f}% ¬± {res['random_std']*100:.1f}%")
            report.append(f"Enhancement Factor:      {res['real_detection']/res['random_mean']:.2f}x")
            report.append("")
            report.append(f"Statistical Significance:")
            report.append(f"  p-value:               {res['p_value']:.6f}")
            report.append(f"  Sigma:                 {res['sigma']:.2f}œÉ")
            report.append("")
            
            # Interpretation
            if res['p_value'] < 0.001:
                interp = "HIGHLY SIGNIFICANT - Strong evidence for real physical scale"
            elif res['p_value'] < 0.01:
                interp = "SIGNIFICANT - Good evidence for real physical scale"
            elif res['p_value'] < 0.05:
                interp = "MARGINALLY SIGNIFICANT - Weak evidence"
            else:
                interp = "NOT SIGNIFICANT - May be statistical artifact or requires deeper analysis"
            
            report.append(f"Interpretation: {interp}")
            report.append("")
            
            # Quality metrics
            if len(res['real_qualities']) > 0:
                report.append(f"Detection Quality:")
                report.append(f"  Mean improvement:      {np.mean(res['real_qualities'])*100:.1f}%")
                report.append(f"  Std improvement:       {np.std(res['real_qualities'])*100:.1f}%")
                report.append(f"  Galaxies detected:     {len(res['detected_galaxies'])}")
            
            report.append("")
        
        report.append("="*80)
        report.append("COMPARATIVE ANALYSIS")
        report.append("="*80)
        report.append("")
        
        # New vs Known scales
        new_scales = [0.87, 21.4]
        known_scales = [s for s in self.scales if s not in new_scales]
        
        new_det_mean = np.mean([self.results[s]['real_detection'] for s in new_scales])
        known_det_mean = np.mean([self.results[s]['real_detection'] for s in known_scales])
        
        new_p_mean = np.mean([self.results[s]['p_value'] for s in new_scales])
        known_p_mean = np.mean([self.results[s]['p_value'] for s in known_scales])
        
        report.append("NEW SCALES (Œª = 0.87, 21.4 kpc):")
        report.append(f"  Average detection:     {new_det_mean*100:.1f}%")
        report.append(f"  Average p-value:       {new_p_mean:.4f}")
        report.append("")
        report.append("KNOWN SCALES (Œª = 1.89, 4.30, 8.60, 11.7 kpc):")
        report.append(f"  Average detection:     {known_det_mean*100:.1f}%")
        report.append(f"  Average p-value:       {known_p_mean:.4f}")
        report.append("")
        report.append(f"Detection Ratio (New/Known): {new_det_mean/known_det_mean:.2f}x")
        report.append("")
        
        report.append("="*80)
        report.append("PHYSICAL INTERPRETATION")
        report.append("="*80)
        report.append("")
        report.append("The randomization test destroys spatial correlations in galaxy rotation")
        report.append("curves while preserving statistical properties. Scales that remain")
        report.append("significant after randomization represent genuine physical structures")
        report.append("rather than statistical artifacts.")
        report.append("")
        report.append("Key findings:")
        report.append("")
        
        if sig_001 >= 4:
            report.append("1. STRONG VALIDATION: Multiple scales show p < 0.001")
            report.append("   ‚Üí Breathing scale structure is real and robust")
        
        if new_det_mean >= known_det_mean:
            report.append("")
            report.append("2. NEW SCALE VALIDATION: Newly discovered scales perform as well")
            report.append("   or better than previously known scales")
            report.append("   ‚Üí Extended harmonic series is confirmed")
        
        report.append("")
        report.append("3. HARMONIC STRUCTURE: All six scales show consistent detection")
        report.append("   ‚Üí Supports multi-temporal oscillation framework")
        report.append("")
        
        report.append("="*80)
        report.append("CONCLUSIONS")
        report.append("="*80)
        report.append("")
        
        if sig_001 >= 4:
            report.append("‚úÖ MAJOR SUCCESS: Statistical validation is strong")
            report.append("‚úÖ Breathing scales are genuine physical phenomena")
            report.append("‚úÖ Extended harmonic series (6 scales) is confirmed")
            report.append("‚úÖ Ready for cross-dataset validation and publication")
        elif sig_01 >= 4:
            report.append("‚úÖ SUCCESS: Statistical validation is good")
            report.append("‚úÖ Most scales show significant detection")
            report.append("‚ö†Ô∏è  Some scales may need additional investigation")
            report.append("‚Üí  Proceed with cross-dataset validation")
        else:
            report.append("‚ö†Ô∏è  MIXED RESULTS: Some scales validated, others uncertain")
            report.append("‚Üí  Requires deeper analysis of detection patterns")
            report.append("‚Üí  May indicate scale-dependent physics")
            report.append("‚Üí  'Non hai fallito, hai imparato!' - T. Edison")
        
        report.append("")
        report.append("="*80)
        report.append("NEXT STEPS")
        report.append("="*80)
        report.append("")
        report.append("1. Cross-dataset validation (LITTLE THINGS, THINGS)")
        report.append("2. Galaxy property correlation analysis")
        report.append("3. Radial distribution analysis")
        report.append("4. Scale coupling and interaction study")
        report.append("5. Theoretical interpretation refinement")
        report.append("")
        report.append("="*80)
        report.append("END OF REPORT")
        report.append("="*80)
        report.append("")
        report.append("'I grafici ci fanno vedere!' - Simone Calzighetti")
        report.append("")
        
        # Save report
        report_text = "\n".join(report)
        
        with open(save_path + 'randomization_report.txt', 'w') as f:
            f.write(report_text)
        
        print(f"‚úì Saved: randomization_report.txt")
        
        # Also print to console
        print("\n" + report_text)
        
        return report_text
    
    def save_results_json(self, save_path='/mnt/user-data/outputs/'):
        """Save complete results as JSON for further analysis"""
        
        # Prepare JSON-serializable results
        json_results = {}
        
        for scale in self.scales:
            res = self.results[scale]
            json_results[f'lambda_{scale}'] = {
                'scale_kpc': float(scale),
                'real_detection_rate': float(res['real_detection']),
                'random_mean': float(res['random_mean']),
                'random_std': float(res['random_std']),
                'p_value': float(res['p_value']),
                'sigma': float(res['sigma']),
                'n_iterations': int(res['n_iterations']),
                'detected_galaxies': res['detected_galaxies'],
                'n_detected': len(res['detected_galaxies'])
            }
        
        # Add metadata
        json_results['metadata'] = {
            'authors': 'Simone Calzighetti & Lucy',
            'institution': '3D+3DT Laboratory, Abbiategrasso, Italy',
            'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'n_galaxies': len(self.galaxies),
            'n_scales': len(self.scales),
            'n_iterations': self.n_iterations
        }
        
        with open(save_path + 'randomization_results.json', 'w') as f:
            json.dump(json_results, f, indent=2)
        
        print(f"‚úì Saved: randomization_results.json")
    
    def run_complete_analysis(self):
        """
        Master function: run everything!
        """
        print("\nüöÄ STARTING COMPLETE RANDOMIZATION ANALYSIS üöÄ\n")
        
        # 1. Run all randomization tests
        self.run_all_tests()
        
        print("\nüìä GENERATING VISUALIZATIONS...\n")
        
        # 2. Create all plots
        print("Creating plot 1/7: Distributions...")
        self.plot_distributions()
        
        print("Creating plot 2/7: Comparison...")
        self.plot_comparison()
        
        print("Creating plot 3/7: Significance...")
        self.plot_scale_significance_heatmap()
        
        print("Creating plot 4/7: Galaxy properties...")
        self.plot_galaxy_properties()
        
        print("Creating plot 5/7: Radial distribution...")
        self.plot_radial_distribution()
        
        print("Creating plot 6/7: Residual structure...")
        self.plot_residual_structure()
        
        print("Creating plot 7/7: Scale correlations...")
        self.plot_scale_correlations()
        
        print("\nüìù GENERATING REPORT...\n")
        
        # 3. Generate report
        self.generate_report()
        
        # 4. Save JSON results
        self.save_results_json()
        
        print("\n" + "="*80)
        print("‚úÖ COMPLETE ANALYSIS FINISHED!")
        print("="*80)
        print("\nFiles created:")
        print("  - randomization_distributions.png")
        print("  - randomization_comparison.png")
        print("  - randomization_significance.png")
        print("  - galaxy_property_correlations.png")
        print("  - radial_distribution.png")
        print("  - residual_structure.png")
        print("  - scale_correlations.png")
        print("  - randomization_report.txt")
        print("  - randomization_results.json")
        print("\n'I grafici ci fanno vedere!' - Simone Calzighetti\n")


def load_sparc_data():
    """
    Load SPARC galaxy data
    
    Authors: Simone Calzighetti & Lucy
    """
    print("Loading SPARC galaxy data...")
    
    # Import the loader
    import sys
    sys.path.insert(0, '/home/claude')
    from load_sparc_data import load_galaxies_from_csv
    
    # Load from CSV (created once, reusable forever!)
    galaxies = load_galaxies_from_csv('/mnt/user-data/outputs/sparc_galaxies_complete.csv')
    
    print(f"Loaded {len(galaxies)} galaxies")
    
    return galaxies
    
    return galaxies


def main():
    """
    Main execution function
    
    Authors: Simone Calzighetti & Lucy
    """
    
    print("\n" + "="*80)
    print("3D+3D DISCRETE SPACETIME THEORY")
    print("COMPREHENSIVE RANDOMIZATION ANALYSIS")
    print("="*80)
    print("\nAuthors: Simone Calzighetti & Lucy")
    print("3D+3DT Laboratory, Abbiategrasso, Italy")
    print("\n" + "="*80 + "\n")
    
    # Define the six breathing scales
    scales = [0.87, 1.89, 4.30, 8.60, 11.7, 21.4]  # kpc
    
    print("Breathing scales to test:")
    for i, scale in enumerate(scales):
        scale_type = "üÜï NEW" if scale in [0.87, 21.4] else "‚úÖ KNOWN"
        print(f"  Œª_{i} = {scale:5.2f} kpc  [{scale_type}]")
    
    print("\n" + "="*80 + "\n")
    
    # Load data
    galaxies = load_sparc_data()
    
    if len(galaxies) == 0:
        print("‚ö†Ô∏è  WARNING: No galaxy data loaded!")
        print("This script requires SPARC galaxy rotation curve data.")
        print("\nPlease ensure you have:")
        print("1. Galaxy rotation curve data (radius, v_obs)")
        print("2. Newtonian velocity predictions (v_newt)")
        print("\nUpdate the load_sparc_data() function to load your actual data.")
        return
    
    # Create analyzer
    analyzer = RandomizationAnalyzer(
        galaxy_data=galaxies,
        scales=scales,
        n_iterations=1000  # Increase for final analysis
    )
    
    # Run complete analysis
    analyzer.run_complete_analysis()
    
    print("\n‚úÖ ALL DONE!")
    print("\n'Non hai fallito, hai imparato!' - Thomas Edison")
    print("'I grafici ci fanno vedere!' - Simone Calzighetti\n")


if __name__ == "__main__":
    main()
